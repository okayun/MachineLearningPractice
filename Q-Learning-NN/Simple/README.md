# 4×4の格子の世界を Q-Learning + Neural Network で学習してみる
"強化学習"のp97?にあるゲームに Q-Learning を適用  
行動価値関数を NN で近似
- ネットワークは 3 層
  - 入力層は状態を表す 2 つの値 (x, y)
  - 中間層はユニット数 4 つの全結合
  - 出力層は上下左右の各行動価値 4 つ
- 学習率は 0.0001
- 重みの更新方法は SGD を使用
- バッチサイズは 1
- リプレイデータなどは使用しない

初期状態を (0, 3) に固定して, 状態 (3, 0) の"下"の行動価値を求めることができるかどうか確認する  
1 度の学習において, 行動回数の上限を 10^6 回に制限  

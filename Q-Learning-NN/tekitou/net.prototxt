layer {
  name: "input_layer"
  type: "MemoryData"
  top: "input"
  top: "dummy1"
  memory_data_param{
    batch_size: 64 # バッチサイズ(プログラムに書いたやつと一致させること)
    channels: 2 # NNの入力の数 位置(x, y)
    height: 1 # 画像の入力とかで使うらしい
    width: 1 # heightと同様
  }
}

layer {
  name: "label_layer"
  type: "MemoryData"
  top: "label"
  top: "dummy2"
  memory_data_param{
    batch_size: 64 # 上と同様
    channels: 4 # NNの出力の数 そのときの行動4つのそれぞれの価値
    height: 1 # 画像の(以下略)
    width: 1 # 上に同じ
  }
}

layer {
  name: "filter_layer"
  type: "MemoryData"
  top: "filter"
  top: "dummy3"
  memory_data_param {
    batch_size: 64
    channels: 4
    height: 1
    width: 1
  }
}

# なんもわからん
layer {
  name: "reshape_layer"
  type: "Reshape"
  bottom: "filter"
  top: "filter_reshaped"
  reshape_param {
    shape {
      dim: 0
      dim: -1
    }
  }
}

# 使い捨て
layer {
  name: "silence_layer"
  type: "Silence"
  bottom: "dummy1"
  bottom: "dummy2"
  bottom: "dummy3"
}

layer {
  name: "inner_product1_layer"
  type: "InnerProduct"
  bottom: "input"
  top: "ip1"
  inner_product_param {
    # 全結合層のユニット数
    # 一般にnクラス分類の際には最後の全結合層のユニット数をnにする．
    num_output: 256
    # 重みの初期化の方法っぽい?
    weight_filler {
      type: "xavier"
    }
    # バイアスの値の指定的なやつ?
    bias_filler {
      type: "constant"
    }
  }
}

layer {
  name: "relu1_layer"
  type: "ReLU"
  bottom: "ip1"
  top: "relu1"
}

layer {
  name: "inner_product2_layer"
  type: "InnerProduct"
  bottom: "relu1"
  top: "ip2"
  inner_product_param {
    num_output: 256
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}

layer {
  name: "relu2_layer"
  type: "ReLU"
  bottom: "ip2"
  top: "relu2"
}

layer {
  name: "inner_product3_layer"
  type: "InnerProduct"
  bottom: "relu2"
  top: "q_values"
  inner_product_param {
    num_output: 4
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}

layer {
  name: "eltwise_layer"
  type: "Eltwise"
  bottom: "q_values"
  bottom: "filter_reshaped"
  top: "filtered_q_values"
  eltwise_param {
    operation: PROD
  }
}

layer {
  name: "loss"
  type: "EuclideanLoss"
  bottom: "filtered_q_values"
  bottom: "label"
  top: "loss"
}
